{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax全连接层使用Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataSet, batch_size=64):\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    else:\n",
    "        num_workers = 4\n",
    "        print('linux')\n",
    "    dataset_iter = torch.utils.data.DataLoader(dataSet, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return dataset_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmaxModel(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(softmaxModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_inputs, num_outputs)\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x.view(x.shape[0], -1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(net, loss, optimizer, n_epochs, train_dataset, test_dataset, batch_size=64):\n",
    "    train_iter = load_dataset(train_dataset, batch_size)\n",
    "    test_iter = load_dataset(test_dataset, batch_size)\n",
    "    for epoch in range(n_epochs+1):\n",
    "        train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for x, lable in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            y = net(x)                 # 前向传播\n",
    "            #lable = lable.to(torch.float)\n",
    "            #lable = lable.view(-1, 1)\n",
    "            \n",
    "            l = loss(y, lable).sum()   # 计算损失\n",
    "            #optimizer.zero_grad()      # 梯度归0\n",
    "            l.backward()               # 后向传播\n",
    "            optimizer.step()           # 更新参数\n",
    "            # print(net.parameters())\n",
    "            \n",
    "            train_loss_sum += l.item()\n",
    "            train_acc_sum += (y.argmax(dim=1) == lable).sum().item()\n",
    "            n += lable.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_loss_sum / n, train_acc_sum / n, test_acc))\n",
    "        \n",
    "def train_ch3(net, train_dataset, test_dataset, loss, num_epochs, batch_size, optimizer=None):\n",
    "    print(\"come in\")\n",
    "    train_iter = load_dataset(train_dataset, batch_size)\n",
    "    test_iter = load_dataset(test_dataset, batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "\n",
    "                \n",
    "            \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0328, -0.0147, -0.0448,  ..., -0.0235, -0.1237,  0.1265],\n",
      "        [ 0.1271, -0.0347, -0.0332,  ..., -0.2823,  0.0621, -0.1517],\n",
      "        [ 0.0393,  0.0320, -0.0233,  ..., -0.0496,  0.0446, -0.1199],\n",
      "        ...,\n",
      "        [-0.1625, -0.0586,  0.0127,  ...,  0.1730,  0.0589,  0.0612],\n",
      "        [-0.0766,  0.0029, -0.0142,  ...,  0.0416, -0.1974,  0.0316],\n",
      "        [-0.0617,  0.0774, -0.0335,  ..., -0.0814,  0.1372,  0.0524]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "come in\n",
      "linux\n",
      "linux\n",
      "epoch 1, loss 0.0100, train acc 0.783, test acc 0.807\n",
      "epoch 2, loss 0.0078, train acc 0.830, test acc 0.821\n",
      "epoch 3, loss 0.0073, train acc 0.839, test acc 0.831\n",
      "epoch 4, loss 0.0071, train acc 0.843, test acc 0.817\n",
      "epoch 5, loss 0.0069, train acc 0.848, test acc 0.831\n",
      "epoch 6, loss 0.0068, train acc 0.850, test acc 0.832\n",
      "epoch 7, loss 0.0067, train acc 0.852, test acc 0.839\n",
      "epoch 8, loss 0.0067, train acc 0.852, test acc 0.821\n",
      "epoch 9, loss 0.0066, train acc 0.855, test acc 0.840\n",
      "epoch 10, loss 0.0065, train acc 0.856, test acc 0.795\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    savePath = './Fashion-MNIST/'\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=savePath, train=True, download=True, transform=transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=savePath, train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    # 初始化模型参数\n",
    "    num_inputs = 784\n",
    "    num_outputs = 10\n",
    "    net = softmaxModel(num_inputs, num_outputs)\n",
    "    init.normal_(net.linear.weight, mean=0, std=0.1)\n",
    "    init.constant_(net.linear.bias, val=1)\n",
    "    for param in net.parameters():\n",
    "        print(param)\n",
    "    n_epochs=10\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = 0.1)\n",
    "    print(optimizer)\n",
    "    train_ch3(net, mnist_train, mnist_test, loss, optimizer=optimizer, batch_size=64, num_epochs=n_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
